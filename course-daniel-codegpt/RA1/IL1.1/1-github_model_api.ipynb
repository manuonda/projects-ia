{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10750b",
   "metadata": {},
   "source": [
    "## OPENAI API con Azure\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "## Instalacion de dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.61.1\n",
      "Python version :  3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n",
      "GITHUB_TOKEN  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\n",
      "Base Url configurada :  https://models.inference.ai.azure.com\n",
      "Api key configurada :  ok\n",
      "Api key preview:  ghp_kmEI0J...ghp_\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "# Verificar que tenemos las bibliotecas correctas \n",
    "print(\"OpenAI library version:\",__import__('openai').__version__)\n",
    "#print(\"Python version : \",__import__('sys').version)\n",
    "#print(\"GITHUB_TOKEN \",os.environ.get(\"GITHUB_TOKEN\"))\n",
    "#Configuration del cliente OpenAI para Github Models\n",
    "try:\n",
    "    #Configurar el cliente con variables de entorno \n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    print(\"Base Url configurada : \",client.base_url)\n",
    "    print(\"Api key configurada : \", \"ok\" if client.api_key else \"x\")\n",
    "    if client.api_key:\n",
    "        print(\"Api key preview: \", client.api_key[:10] + \"...\"+ client.api_key[:4])\n",
    "    else:\n",
    "        print(\"Api key no encontrada. Asegurate de configurar GITHUB_TOKEN\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(f\"Error en configuration: {ex}\")\n",
    "    print(\"Verifica que las variables de entorno esten configuradas correctamente\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cb86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta del Modelo ====\n",
      "¬°Hola! Estoy bien, gracias por preguntar. üòä\n",
      "\n",
      "==== Informacion tecnica =====\n",
      "\n",
      "choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='¬°Hola! Estoy bien, gracias por preguntar. üòä', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})]\n",
      "\n",
      "Modelo usado: gpt-4o-2024-11-20\n",
      "\n",
      "Tokens usado: 31\n",
      "\n",
      "Tokens de entrada: 19\n",
      "\n",
      "Tokens de salidad: 12\n"
     ]
    }
   ],
   "source": [
    "#Primera llamada basica al model \n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\":\"user\", \"content\":\"Hola ¬øComo estas?Responde en una oracion \"}\n",
    "                \n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        print(\"=== Respuesta del Modelo ====\")\n",
    "        print(response.choices[0].message.content)\n",
    "        #print(response)\n",
    "        print(f\"\\n==== Informacion tecnica =====\")\n",
    "        print(f\"\\nchoices: {response.choices}\")\n",
    "        print(f\"\\nModelo usado: {response.model}\")\n",
    "        print(f\"\\nTokens usado: {response.usage.total_tokens}\")\n",
    "        print(f\"\\nTokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"\\nTokens de salidad: {response.usage.completion_tokens}\")\n",
    "       \n",
    "     \n",
    "    except Exception as ex:\n",
    "        print(f\"Error en la llamada :{ex}\")\n",
    "        print(\"Verifica tu configuracion y conexion a internet\")\n",
    "        \n",
    "#Ejecutar la funcion \n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8979803",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dcfae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Respuesta con Mensaje de Sistema ====\n",
      "¬°Claro! Imag√≠nate que una API es como un mesero en un restaurante. T√∫ (el cliente) no necesitas saber c√≥mo se cocina la comida en la cocina, pero s√≠ sabes qu√© quieres pedir del men√∫. Entonces, le dices al mesero tu orden, √©l la lleva a la cocina, y luego te trae lo que pediste.\n",
      "\n",
      "Ahora, llevemos esta idea al mundo de la tecnolog√≠a:\n",
      "\n",
      "### ¬øQu√© es una API?\n",
      "Una **API** (Interfaz de Programaci√≥n de Aplicaciones, por sus siglas en ingl√©s) es una forma en que diferentes programas o sistemas se comunican entre s√≠. Es como un puente o intermediario que permite que una aplicaci√≥n pida informaci√≥n o servicios a otra aplicaci√≥n o sistema, sin que necesiten saber c√≥mo funcionan internamente.\n",
      "\n",
      "### Ejemplo pr√°ctico:\n",
      "Supongamos que quieres usar una aplicaci√≥n de clima en tu tel√©fono para saber si va a llover. Esa app no tiene toda la informaci√≥n del clima por s√≠ sola. Lo que hace\n"
     ]
    }
   ],
   "source": [
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\" :\"system\",\n",
    "                    \"content\": \"Eres un experto en tecnolog√≠a que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos pr√°cticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":\"¬øQue es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        print(\"==== Respuesta con Mensaje de Sistema ====\")\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "    \n",
    "usar_mensaje_sistema()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988525",
   "metadata": {},
   "source": [
    "## Explorando con Parametros de Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_temperatura():\n",
    "    prompt =\"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1,0.5,0.6]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n {'='*50}\")\n",
    "        print(f\"\\nTemperature : {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "               model=\"gpt-4o\",\n",
    "               messages=[{'role':\"user\", \"content\": prompt}],\n",
    "               temperature=temp,\n",
    "               max_tokens=100\n",
    "            )\n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "        except Exception as ex:\n",
    "           print(f\"Error: {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bfdac",
   "metadata": {},
   "source": [
    "## Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el c√≥digo para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentaci√≥n de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Dise√±a un mensaje de sistema para crear un asistente especializado en un tema espec√≠fico (ejemplo: finanzas, salud, educaci√≥n).\n",
    "\n",
    "### Ejercicio 3: Optimizaci√≥n de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuraci√≥n segura** de APIs usando variables de entorno\n",
    "2. **Par√°metros b√°sicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos c√≥mo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas m√°s poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "741b9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========\n",
      "\n",
      " model : gpt-4o\n",
      "\n",
      "response: ¬°Claro que s√≠! Estoy aqu√≠ para ayudarte con cualquier consulta financiera que tengas. Ya sea que necesites asesoramiento sobre inversiones, planificaci√≥n financiera, presupuestos, impuestos, ahorro, an√°lisis de mercados o cualquier otro tema relacionado con finanzas, no dudes en preguntar. ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "===========\n",
      "\n",
      " model : gpt-4o-mini\n",
      "\n",
      "response: ¬°Claro! Estoy aqu√≠ para ayudarte con cualquier pregunta o tema relacionado con finanzas. Ya sea que necesites informaci√≥n sobre inversiones, ahorro, planificaci√≥n financiera, presupuestos o cualquier otro aspecto financiero, no dudes en preguntar. ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "===========\n",
      "\n",
      " model : DeepSeek-R1-0528\n",
      "\n",
      "response: <think>\n",
      "¬°Vaya! El usuario simplemente declar√≥ que soy un experto en finanzas sin hacer una pregunta espec√≠fica. Esto es interesante porque podr√≠a significar varias cosas:\n",
      "\n",
      "Primero, quiz√°s est√° probando mis credenciales o verificando que realmente entiendo del tema. Tal vez tuvo malas experiencias anteriores con asistentes que no eran realmente expertos. \n",
      "\n",
      "Tambi√©n podr√≠a ser que est√° preparando una consulta m√°s compleja y quiere asegurarse de que estoy calificado antes de invertir tiempo en explicar su situaci√≥n. O simplemente podr√≠a ser un mensaje introductorio antes de plantear su duda real.\n",
      "\n",
      "Como no dio contexto, lo mejor ser√°:\n",
      "- Confirmar mi especializaci√≥n para generar confianza\n",
      "- Dar un abanico de posibles √°reas donde puedo ayudar\n",
      "- Invitarle a especificar su necesidad\n",
      "\n",
      "Mejor evitar suponer su nivel de conocimiento financiero - podr√≠a ser desde un estudiante hasta un\n"
     ]
    }
   ],
   "source": [
    "def asistente_especializado():\n",
    "    models = [\"gpt-4o\", \"gpt-4o-mini\", \"DeepSeek-R1-0528\"]\n",
    "    prompt = \"Eres un asistente experto en finanzas\"\n",
    "    try:\n",
    "        for mll in models:\n",
    "            response = client.chat.completions.create(\n",
    "                model=mll,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\", \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            print(f\"\\n===========\")\n",
    "            print(f\"\\n model : {mll}\")\n",
    "            print(f\"\\nresponse: {response.choices[0].message.content}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"\\n Asistente Especializado : {ex}\")\n",
    "\n",
    "\n",
    "asistente_especializado()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
