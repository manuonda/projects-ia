{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10750b",
   "metadata": {},
   "source": [
    "## OPENAI API con Azure\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "## Instalacion de dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9f1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.97.0\n",
      "Python version :  3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0]\n",
      "GITHUB_TOKEN  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\n",
      "Base Url configurada :  https://models.inference.ai.azure.com\n",
      "Api key configurada :  ok\n",
      "Api key preview:  ghp_zoYw8j...ghp_\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "# Verificar que tenemos las bibliotecas correctas \n",
    "print(\"OpenAI library version:\",__import__('openai').__version__)\n",
    "print(\"Python version : \",__import__('sys').version)\n",
    "print(\"GITHUB_TOKEN \",os.environ.get(\"GITHUB_TOKEN\"))\n",
    "#Configuration del cliente OpenAI para Github Models\n",
    "try:\n",
    "    #Configurar el cliente con variables de entorno \n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    print(\"Base Url configurada : \",client.base_url)\n",
    "    print(\"Api key configurada : \", \"ok\" if client.api_key else \"x\")\n",
    "    if client.api_key:\n",
    "        print(\"Api key preview: \", client.api_key[:10] + \"...\"+ client.api_key[:4])\n",
    "    else:\n",
    "        print(\"Api key no encontrada. Asegurate de configurar GITHUB_TOKEN\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(f\"Error en configuration: {ex}\")\n",
    "    print(\"Verifica que las variables de entorno esten configuradas correctamente\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cb86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta del Modelo ====\n",
      "¡Hola! Estoy aquí y listo para ayudarte.\n",
      "\n",
      "==== Informacion tecnica =====\n",
      "\n",
      "choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='¡Hola! Estoy aquí y listo para ayudarte.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})]\n",
      "\n",
      "Modelo usado: gpt-4o-2024-11-20\n",
      "\n",
      "Tokens usado: 30\n",
      "\n",
      "Tokens de entrada: 19\n",
      "\n",
      "Tokens de salidad: 11\n"
     ]
    }
   ],
   "source": [
    "#Primera llamada basica al model \n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\":\"user\", \"content\":\"Hola ¿Como estas?Responde en una oracion \"}\n",
    "                \n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        print(\"=== Respuesta del Modelo ====\")\n",
    "        print(response.choices[0].message.content)\n",
    "        #print(response)\n",
    "        print(f\"\\n==== Informacion tecnica =====\")\n",
    "        print(f\"\\nchoices: {response.choices}\")\n",
    "        print(f\"\\nModelo usado: {response.model}\")\n",
    "        print(f\"\\nTokens usado: {response.usage.total_tokens}\")\n",
    "        print(f\"\\nTokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"\\nTokens de salidad: {response.usage.completion_tokens}\")\n",
    "       \n",
    "     \n",
    "    except Exception as ex:\n",
    "        print(f\"Error en la llamada :{ex}\")\n",
    "        print(\"Verifica tu configuracion y conexion a internet\")\n",
    "        \n",
    "#Ejecutar la funcion \n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8979803",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dcfae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Respuesta con Mensaje de Sistema ====\n",
      "¡Claro! Imagínate que una API es como un mesero en un restaurante. Tú (el cliente) no necesitas saber cómo se cocina la comida en la cocina, pero sí sabes qué quieres pedir del menú. Entonces, le dices al mesero tu orden, él la lleva a la cocina, y luego te trae lo que pediste.\n",
      "\n",
      "Ahora, llevemos esta idea al mundo de la tecnología:\n",
      "\n",
      "### ¿Qué es una API?\n",
      "Una **API** (Interfaz de Programación de Aplicaciones, por sus siglas en inglés) es una forma en que diferentes programas o sistemas se comunican entre sí. Es como un puente o intermediario que permite que una aplicación pida información o servicios a otra aplicación o sistema, sin que necesiten saber cómo funcionan internamente.\n",
      "\n",
      "### Ejemplo práctico:\n",
      "Supongamos que quieres usar una aplicación de clima en tu teléfono para saber si va a llover. Esa app no tiene toda la información del clima por sí sola. Lo que hace\n"
     ]
    }
   ],
   "source": [
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\" :\"system\",\n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":\"¿Que es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        print(\"==== Respuesta con Mensaje de Sistema ====\")\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "    \n",
    "usar_mensaje_sistema()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988525",
   "metadata": {},
   "source": [
    "## Explorando con Parametros de Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_temperatura():\n",
    "    prompt =\"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1,0.5,0.6]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n {'='*50}\")\n",
    "        print(f\"\\nTemperature : {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "               model=\"gpt-4o\",\n",
    "               messages=[{'role':\"user\", \"content\": prompt}],\n",
    "               temperature=temp,\n",
    "               max_tokens=100\n",
    "            )\n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "        except Exception as ex:\n",
    "           print(f\"Error: {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bfdac",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========\n",
      "\n",
      " model : gpt-4o\n",
      "\n",
      "response: ¡Gracias por informarme! Estoy aquí para ayudarte con cualquier pregunta o consulta relacionada con finanzas personales, inversiones, presupuestos, planificación financiera, análisis de mercados, contabilidad, impuestos, criptomonedas o cualquier otro tema financiero. ¿En qué puedo asistirte hoy?\n",
      "\n",
      "===========\n",
      "\n",
      " model : gpt-4o-mini\n",
      "\n",
      "response: ¡Claro! Estoy aquí para ayudarte con cualquier pregunta o tema relacionado con finanzas. ¿En qué puedo asistirte hoy?\n",
      "\n",
      "===========\n",
      "\n",
      " model : DeepSeek-R1-0528\n",
      "\n",
      "response: El usuario está buscando información sobre cómo calcular el riesgo de crédito en una cartera de préstamos. Necesita una explicación detallada de los métodos y modelos utilizados, así como ejemplos prácticos.     \n",
      "\n",
      "Primero, debo entender que el usuario quiere una guía completa sobre la evaluación del riesgo crediticio a nivel de cartera, no solo a nivel individual. Esto implica conceptos como pérdida esperada, diversificación, correlación, y modelos como CreditMetrics o CreditRisk+.\n",
      "\n",
      "Recuerdo que el riesgo de crédito en cartera considera cómo los préstamos interactúan entre sí. La correlación es clave aquí porque si todos los préstamos fallan juntos, el riesgo es mayor. Debo explicar métricas como la Pérdida Esperada (EL), Pérdida Inesperada (UL), y Value at Risk (VaR).\n",
      "\n",
      "Voy a estructurar mi respuesta en partes: primero los concept\n"
     ]
    }
   ],
   "source": [
    "def asistente_especializado():\n",
    "    models = [\"gpt-4o\", \"gpt-4o-mini\", \"DeepSeek-R1-0528\"]\n",
    "    prompt = \"Eres un asistente experto en finanzas\"\n",
    "    try:\n",
    "        for mll in models:\n",
    "            response = client.chat.completions.create(\n",
    "                model=mll,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\", \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            print(f\"\\n===========\")\n",
    "            print(f\"\\n model : {mll}\")\n",
    "            print(f\"\\nresponse: {response.choices[0].message.content}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"\\n Asistente Especializado : {ex}\")\n",
    "\n",
    "\n",
    "asistente_especializado()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "il1-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
